{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: PREPROCESSING -> Get and read data from DementiaBank .CHA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python lib to read CHA files from DementiaBank\n",
    "import pylangacq as pla\n",
    "import os\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "#downloaded CHA files from PittCorpus, Baycrest, Kempler, Deleware, Holland, Lu, WLS\n",
    "\n",
    "cha_files_main_dir = r\"C:\\Users\\Gianna\\OneDrive\\Desktop\\CHA Files\"\n",
    "dementia_cha_path = r\"C:\\Users\\Gianna\\OneDrive\\Desktop\\CHA Files\\Dementia\"\n",
    "mci_cha_path = r\"C:\\Users\\Gianna\\OneDrive\\Desktop\\CHA Files\\MCI\"\n",
    "control_cha_path = r\"C:\\Users\\Gianna\\OneDrive\\Desktop\\CHA Files\\Control\"\n",
    "uncategorized_cha = []\n",
    "uncategorized_cha = [os.path.join(cha_files_main_dir, file) for file in os.listdir(cha_files_main_dir) if os.path.isfile(os.path.join(cha_files_main_dir, file))]\n",
    "\n",
    "# dementia patients data\n",
    "dementia_files = [file for file in os.listdir(dementia_cha_path) if os.path.isfile(os.path.join(dementia_cha_path, file))]\n",
    "dementia_folders = [folder for folder in os.listdir(dementia_cha_path) if os.path.isdir(os.path.join(dementia_cha_path, folder))]\n",
    "dementia_files_all = [os.path.join(dementia_cha_path, folder, file) for folder in dementia_folders for file in os.listdir(os.path.join(dementia_cha_path, folder))]\n",
    "\n",
    "# mci data\n",
    "mci_files = [os.path.join(mci_cha_path, file) for file in os.listdir(mci_cha_path)]\n",
    "mci_files += uncategorized_cha\n",
    "\n",
    "#control\n",
    "control_files = [file for file in os.listdir(control_cha_path) if os.path.isfile(os.path.join(control_cha_path, file))]\n",
    "control_folders = [folder for folder in os.listdir(control_cha_path) if os.path.isdir(os.path.join(control_cha_path, folder))]\n",
    "control_files_all = [os.path.join(control_cha_path, folder, file) for folder in control_folders for file in os.listdir(os.path.join(control_cha_path, folder))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CHAT files for all categories\n",
    "AD_chats = pla.Reader.from_files(dementia_files_all)\n",
    "MCI_chats = pla.Reader.from_files(mci_files)\n",
    "Control_chats = pla.Reader.from_files(control_files_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read chats, use header to filter the participant and condition of participant \n",
    "AD_headers = pla.Reader.headers(AD_chats)\n",
    "MCI_headers = pla.Reader.headers(MCI_chats)\n",
    "control_headers = pla.Reader.headers(Control_chats)\n",
    "\n",
    "# create lists of participant info (dictionaries) for each group\n",
    "AD_participants = []\n",
    "MCI_participants = []\n",
    "Control_participants = []\n",
    "\n",
    "# all chats in correct category for each group\n",
    "AD_par_chats = []\n",
    "MCI_par_chats = []\n",
    "Control_par_chats = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to ensure that all chats are in the correct catgories\n",
    "# read chats, use file header to filter the participant and condition of participant \n",
    "def categorize_header(headers, chats):\n",
    "    # fix categorization of patients based on headers\n",
    "    for num_par, header in enumerate(headers):\n",
    "        if 'Participants' in header.keys():\n",
    "            patient_dict = header['Participants']['PAR']\n",
    "            patient_group = patient_dict['group'].lower()\n",
    "\n",
    "            if 'ad' in patient_group or \"alzheimer's\" in patient_group:\n",
    "                AD_participants.append(patient_dict)\n",
    "                AD_par_chats.append(chats[num_par])\n",
    "\n",
    "            elif 'mci' in patient_group:\n",
    "                MCI_participants.append(patient_dict)\n",
    "                MCI_par_chats.append(chats[num_par])\n",
    "\n",
    "            elif 'control' in patient_group or patient_dict['corpus'] == 'WLS':\n",
    "                Control_participants.append(patient_dict)\n",
    "                Control_par_chats.append(chats[num_par])\n",
    "\n",
    "\n",
    "categorize_header(AD_headers, AD_chats)\n",
    "categorize_header(MCI_headers, MCI_chats)\n",
    "categorize_header(control_headers, Control_chats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846 AD patients, 213 MCI patients, and 877 Control subjects\n"
     ]
    }
   ],
   "source": [
    "# check how many participants/samples in each group\n",
    "print(f'{len(AD_participants)} AD patients, {len(MCI_participants)} MCI patients, and {len(Control_participants)} Control subjects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get words from chats and perform preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['mhm',\n",
       "   '.',\n",
       "   'alright',\n",
       "   '.',\n",
       "   \"there's\",\n",
       "   'a',\n",
       "   'young',\n",
       "   'boy',\n",
       "   \"that's\",\n",
       "   'getting',\n",
       "   'a',\n",
       "   'cookie',\n",
       "   'jar',\n",
       "   '.',\n",
       "   'and',\n",
       "   \"he's\",\n",
       "   'in',\n",
       "   'bad',\n",
       "   'shape',\n",
       "   'because',\n",
       "   'the',\n",
       "   'thing',\n",
       "   'is',\n",
       "   'falling',\n",
       "   'over',\n",
       "   '.',\n",
       "   'and',\n",
       "   'in',\n",
       "   'the',\n",
       "   'picture',\n",
       "   'the',\n",
       "   'mother',\n",
       "   'is',\n",
       "   'washing',\n",
       "   'dishes',\n",
       "   'and',\n",
       "   \"doesn't\",\n",
       "   'see',\n",
       "   'it',\n",
       "   '.',\n",
       "   'and',\n",
       "   'so',\n",
       "   'the',\n",
       "   'water',\n",
       "   'is',\n",
       "   'overflowing',\n",
       "   'in',\n",
       "   'the',\n",
       "   'sink',\n",
       "   '.',\n",
       "   'and',\n",
       "   'the',\n",
       "   'dishes',\n",
       "   'might',\n",
       "   'fall',\n",
       "   'over',\n",
       "   'there',\n",
       "   'if',\n",
       "   'you',\n",
       "   \"don't\",\n",
       "   'get',\n",
       "   'it',\n",
       "   '.',\n",
       "   'and',\n",
       "   \"it's\",\n",
       "   'a',\n",
       "   'picture',\n",
       "   'of',\n",
       "   'a',\n",
       "   'kitchen',\n",
       "   'window',\n",
       "   '.',\n",
       "   'and',\n",
       "   'the',\n",
       "   'curtains',\n",
       "   'are',\n",
       "   'very',\n",
       "   'distinct',\n",
       "   '.',\n",
       "   'but',\n",
       "   'the',\n",
       "   'water',\n",
       "   'is',\n",
       "   'still',\n",
       "   'flowing',\n",
       "   '.']]]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all sentences from all AD participants\n",
    "AD_words = [AD_chat.words(participants='PAR', by_files=True) for AD_chat in AD_par_chats]\n",
    "\n",
    "# all MCI files from participants \n",
    "MCI_words = [MCI_chat.words(participants='PAR', by_files=True) for MCI_chat in MCI_par_chats]\n",
    "\n",
    "# all control sentences\n",
    "control_words = [Control_chat.words(participants='PAR', by_files=True) for Control_chat in Control_par_chats]\n",
    "\n",
    "AD_words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mhm',\n",
       " '.',\n",
       " 'alright',\n",
       " '.',\n",
       " 'theres',\n",
       " 'a',\n",
       " 'young',\n",
       " 'boy',\n",
       " 'thats',\n",
       " 'getting',\n",
       " 'a',\n",
       " 'cookie',\n",
       " 'jar',\n",
       " '.',\n",
       " 'and',\n",
       " 'hes',\n",
       " 'in',\n",
       " 'bad',\n",
       " 'shape',\n",
       " 'because',\n",
       " 'the',\n",
       " 'thing',\n",
       " 'is',\n",
       " 'falling',\n",
       " 'over',\n",
       " '.',\n",
       " 'and',\n",
       " 'in',\n",
       " 'the',\n",
       " 'picture',\n",
       " 'the',\n",
       " 'mother',\n",
       " 'is',\n",
       " 'washing',\n",
       " 'dishes',\n",
       " 'and',\n",
       " 'doesnt',\n",
       " 'see',\n",
       " 'it',\n",
       " '.',\n",
       " 'and',\n",
       " 'so',\n",
       " 'the',\n",
       " 'water',\n",
       " 'is',\n",
       " 'overflowing',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sink',\n",
       " '.',\n",
       " 'and',\n",
       " 'the',\n",
       " 'dishes',\n",
       " 'might',\n",
       " 'fall',\n",
       " 'over',\n",
       " 'there',\n",
       " 'if',\n",
       " 'you',\n",
       " 'dont',\n",
       " 'get',\n",
       " 'it',\n",
       " '.',\n",
       " 'and',\n",
       " 'its',\n",
       " 'a',\n",
       " 'picture',\n",
       " 'of',\n",
       " 'a',\n",
       " 'kitchen',\n",
       " 'window',\n",
       " '.',\n",
       " 'and',\n",
       " 'the',\n",
       " 'curtains',\n",
       " 'are',\n",
       " 'very',\n",
       " 'distinct',\n",
       " '.',\n",
       " 'but',\n",
       " 'the',\n",
       " 'water',\n",
       " 'is',\n",
       " 'still',\n",
       " 'flowing',\n",
       " '.']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove extra quotation marks around words \n",
    "AD_words = [' '.join(sentence).replace('\"', '').replace(\"'\", '').split(' ') for par_chat in AD_words for sentence in par_chat]\n",
    "MCI_words = [' '.join(sentence).replace('\"', '').replace(\"'\", '').split(' ') for par_chat in MCI_words for sentence in par_chat]\n",
    "control_words = [' '.join(sentence).replace('\"', '').replace(\"'\", '').split(' ') for par_chat in control_words for sentence in par_chat]\n",
    "AD_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: FEATURE TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I test methods that have been used in past research to distinguish AD samples from non AD samples. Particularly, methods tried are found in a paper by Fraser found here: https://www.cs.toronto.edu/~kfraser/Fraser15-JAD.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from nltk.corpus import words\n",
    "import itertools\n",
    "\n",
    "eng_corpus = set(words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Comparing POS tags\n",
    "\n",
    "One of the elements that has been observed by some in the semantics of dementia patients is a higher frequency of verbs, pronouns, and adjectives compared to nouns. In the following function, I compute the mean of the ratios between pronoun : noun, verb : noun, and adjective : noun for all chats. I then found the mean ratios of the means of all chats and compared between groups.  \n",
    "\n",
    "Prediction: The greater the ratio returned, the more likely a set of samples will be in the AD/MCI group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_ratios(chats):\n",
    "    # create an array that stores the average ratio of the ratios -> pronoun: noun, verb: noun, adj: noun\n",
    "    # ratios closer or greater than 1 indicate higher AD likelihood \n",
    "    chat_ratios = []\n",
    "    for chat in chats:\n",
    "        num_noun = 0\n",
    "        num_pronoun = 0\n",
    "        num_verb = 0\n",
    "        num_adj = 0\n",
    "        tokens = chat.tokens(participants='PAR', by_utterances=True)\n",
    "        for token in tokens:\n",
    "            for utterance in token:\n",
    "                # get pos from MOR grammar \n",
    "                mor_tier = utterance.to_mor_tier().split('|')\n",
    "                pos = mor_tier[0]\n",
    "                if 'n' in pos:\n",
    "                    num_noun += 1\n",
    "                if 'v' in pos:\n",
    "                    num_verb += 1\n",
    "                if 'adj' in pos:\n",
    "                    num_adj += 1\n",
    "                if 'pro' in pos:\n",
    "                    num_pronoun += 1\n",
    "        if num_noun != 0:\n",
    "            # add the ratios from this chat to all ratios \n",
    "            chat_ratios.append(np.mean([num_pronoun/num_noun, num_verb/num_noun, num_adj/num_noun]))\n",
    "        else:\n",
    "            chat_ratios.append(0)\n",
    "    return chat_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average noun-noun to noun ratios for each group ->\n",
      "AD: 0.39824405262825024\n",
      "MCI: 0.33220975210115683\n",
      "Control: 0.2935963541446591\n"
     ]
    }
   ],
   "source": [
    "noun_ratio_AD = np.mean(get_pos_ratios(AD_par_chats))\n",
    "noun_ratio_MCI = np.mean(get_pos_ratios(MCI_par_chats))\n",
    "noun_ratio_control = np.mean(get_pos_ratios(Control_par_chats))\n",
    "\n",
    "print(f'Average noun-noun to noun ratios for each group ->\\nAD: {noun_ratio_AD}\\nMCI: {noun_ratio_MCI}\\nControl: {noun_ratio_control}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the ratio of non-nouns : nouns is in fact higher in AD patients. To confirm, use a smaller amount of 150 random samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average noun-noun to noun ratios for each group ->\n",
      "AD: 0.42044305618618194\n",
      "MCI: 0.3247101179608221\n",
      "Control: 0.30215984180620725\n"
     ]
    }
   ],
   "source": [
    "ad_ratios = np.mean(get_pos_ratios(random.sample(AD_par_chats, 150)))\n",
    "mci_ratios = np.mean(get_pos_ratios(random.sample(MCI_par_chats, 150)))\n",
    "control_ratios = np.mean(get_pos_ratios(random.sample(Control_par_chats, 150)))\n",
    "\n",
    "print(f'Average noun-noun to noun ratios for each group ->\\nAD: {ad_ratios}\\nMCI: {mci_ratios}\\nControl: {control_ratios}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that this test can be used as an appropriate measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another element that can be used to test lexical diversity is repetetive phrases or words. Here, I use NLTK corpus to find non-words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Comparing non-word ratios\n",
    "\n",
    "Prediction: A higher non-word frequency is correlated with AD.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio to find number of nonwords in sentences \n",
    "def ratio_nonwords(chat_words):\n",
    "    nonwords = 0\n",
    "    total_words = 0\n",
    "    # collapse 3d chats to 1d list \n",
    "    chat_words_flattened = itertools.chain.from_iterable(itertools.chain.from_iterable(chat_words))\n",
    "    for word in chat_words_flattened:\n",
    "        # check if in nltk corpus\n",
    "        if word not in eng_corpus:\n",
    "            nonwords += 1\n",
    "        total_words += 1\n",
    "    return (nonwords / total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratios of nonwords for each group ->\n",
      "AD: 0.07079032942174467\n",
      "MCI: 0.044237933194383905\n",
      "Control: 0.03980657587495594\n"
     ]
    }
   ],
   "source": [
    "ad_nonwords = ratio_nonwords(AD_words)\n",
    "mci_nonwords = ratio_nonwords(MCI_words)\n",
    "control_nonwords = ratio_nonwords(control_words)\n",
    "\n",
    "print(f'Ratios of nonwords for each group ->\\nAD: {ad_nonwords}\\nMCI: {mci_nonwords}\\nControl: {control_nonwords}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm with 150 random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratios of nonwords for each group ->\n",
      "AD: 0.063695006881913\n",
      "MCI: 0.0451481999396781\n",
      "Control: 0.03952134994273962\n"
     ]
    }
   ],
   "source": [
    "ad_nonwords = ratio_nonwords(random.sample(AD_words, 150))\n",
    "mci_nonwords = ratio_nonwords(random.sample(MCI_words, 150))\n",
    "control_nonwords = ratio_nonwords(random.sample(control_words, 150))\n",
    "\n",
    "print(f'Ratios of nonwords for each group ->\\nAD: {ad_nonwords}\\nMCI: {mci_nonwords}\\nControl: {control_nonwords}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Finding repetetives stem of unique words / total\n",
    "Prediction: AD patients will use less unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Measuring non-specific speech with word biases\n",
    "Prediction: Those with AD have a bias to reusing words that are shorter in length and higher frequency words, especially verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Measuring vocabulary richness -> honore statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Syntactic Analysis: CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 frequency\n",
    "unigrams and excluded binary unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8. Incomplete sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9. Filler and question word ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.10. Word entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.11. Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing participant ages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
